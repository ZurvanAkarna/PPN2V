{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f23a4357",
   "metadata": {},
   "source": [
    "# üß¨ N2V Experiment Pipeline - Multi-Seed Training with PSNR Tracking\n",
    "\n",
    "**Experimental Setup:**\n",
    "- Single noisy 128√ó128√ó1 image\n",
    "- Patch masking: 7√ó7 blind spots, 10% mask ratio, zero replacement\n",
    "- MSE loss computed ONLY on masked pixels\n",
    "- U-Net: 4 base channels, depth 3, ReLU, Kaiming init\n",
    "- Training: up to 150 epochs, lr=1e-3\n",
    "- Per-epoch checkpoints saved\n",
    "- Best epoch selected by PSNR (if ground truth available)\n",
    "- Multiple independent seed runs with logging\n",
    "\n",
    "---\n",
    "\n",
    "## Experiment Configuration\n",
    "\n",
    "| Parameter | Value | Description |\n",
    "|-----------|-------|-------------|\n",
    "| Image Size | 128√ó128√ó1 | Single channel |\n",
    "| Mask Patch | 7√ó7 | Blind spot size |\n",
    "| Mask Ratio | 10% | Pixels masked per patch |\n",
    "| Replacement | Zero | Masked pixel value |\n",
    "| U-Net Channels | 4 | Base width (very small) |\n",
    "| U-Net Depth | 3 | Encoder/decoder levels |\n",
    "| Activation | ReLU | Default in UNet |\n",
    "| Weight Init | Kaiming | He initialization |\n",
    "| Learning Rate | 1e-3 | Adam optimizer |\n",
    "| Max Epochs | 150 | Training limit |\n",
    "| Num Seeds | 5 | Independent runs |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c768a6",
   "metadata": {},
   "source": [
    "## üìÅ Section 1: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47b84e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MOUNT DRIVE & BASIC IMPORTS\n",
    "# ============================================================\n",
    "# These imports are needed early for data loading\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tifffile\n",
    "\n",
    "# Define paths - SEPARATE DIRECTORIES for N2V and PN2V results\n",
    "DRIVE_PROJECT_PATH = '/content/drive/MyDrive/PPN2V'\n",
    "\n",
    "# ========== DATA PATH - YOUR DATASET_01 FOLDER ==========\n",
    "DRIVE_DATA_PATH = '/content/drive/MyDrive/PPN2V/DATASET_01'\n",
    "\n",
    "# Stage 1: N2V results\n",
    "N2V_RESULTS_PATH = '/content/drive/MyDrive/PPN2V/results/DATASET_01/n2v_optimized'\n",
    "\n",
    "# Stage 2: PN2V results  \n",
    "PN2V_RESULTS_PATH = '/content/drive/MyDrive/PPN2V/results/DATASET_01/pn2v_bootstrap'\n",
    "\n",
    "# Final comparison\n",
    "COMPARISON_PATH = '/content/drive/MyDrive/PPN2V/results/DATASET_01/comparison'\n",
    "\n",
    "# Create all directories\n",
    "for path in [N2V_RESULTS_PATH, PN2V_RESULTS_PATH, COMPARISON_PATH]:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "print(f\"‚úì Drive mounted\")\n",
    "print(f\"\\nüìÅ Data Path:     {DRIVE_DATA_PATH}\")\n",
    "print(f\"üìÅ Output Directories:\")\n",
    "print(f\"  N2V Results:    {N2V_RESULTS_PATH}\")\n",
    "print(f\"  PN2V Results:   {PN2V_RESULTS_PATH}\")\n",
    "print(f\"  Comparison:     {COMPARISON_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616724bc",
   "metadata": {},
   "source": [
    "## üì• Section 2: Clone Repository & Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63ba617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Clone or update repository\n",
    "REPO_PATH = '/content/PPN2V'\n",
    "GITHUB_REPO = 'https://github.com/ZurvanAkarna/PPN2V.git'\n",
    "\n",
    "if os.path.exists(REPO_PATH):\n",
    "    print(\"Repository exists, pulling latest changes...\")\n",
    "    os.chdir(REPO_PATH)\n",
    "    subprocess.run(['git', 'pull'], check=True)\n",
    "else:\n",
    "    print(\"Cloning repository...\")\n",
    "    subprocess.run(['git', 'clone', GITHUB_REPO, REPO_PATH], check=True)\n",
    "    os.chdir(REPO_PATH)\n",
    "\n",
    "# Install the package\n",
    "print(\"\\nInstalling PPN2V package...\")\n",
    "subprocess.run([sys.executable, '-m', 'pip', 'install', '-e', '.', '-q'], check=True)\n",
    "\n",
    "# Install additional dependencies\n",
    "subprocess.run([sys.executable, '-m', 'pip', 'install', 'tifffile', 'scikit-image', '-q'], check=True)\n",
    "\n",
    "print(\"\\n‚úì Installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c9496a",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Section 3: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961bf793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üìã EXPERIMENT CONFIGURATION\n",
    "# ============================================================\n",
    "# Requirements:\n",
    "# - 128√ó128√ó1 image, mask_patch=7, mask_ratio=0.1, zero replacement\n",
    "# - U-Net: C=4 base channels, depth=3, ReLU, Kaiming init\n",
    "# - lr=1e-3, max 150 epochs\n",
    "# - Per-epoch checkpoints, PSNR-based best selection\n",
    "# - Multiple seed runs with logging\n",
    "\n",
    "CONFIG = {\n",
    "    # Data settings\n",
    "    'data_name': 'DATASET_01',\n",
    "    'data_file': 'noisy_image_jitter_skips_0__0_3_flags_0__0_4_Gaussian_0.6.tif',\n",
    "    'ground_truth_file': 'jittered_image.tif',  # GT for PSNR tracking\n",
    "\n",
    "    # ========== EXPERIMENT PARAMETERS ==========\n",
    "    # Network architecture (YOUR REQUIREMENTS)\n",
    "    'net_depth': 3,                     # U-Net depth\n",
    "    'net_start_filts': 4,               # Base channels C=4 (YOUR REQUIREMENT)\n",
    "\n",
    "    # Masking parameters (YOUR REQUIREMENTS)\n",
    "    'mask_patch_size': 7,               # 7√ó7 blind spot patches\n",
    "    'mask_ratio': 0.10,                 # 10% pixels masked (YOUR REQUIREMENT)\n",
    "\n",
    "    # Training parameters (YOUR REQUIREMENTS)\n",
    "    'n2v_max_epochs': 150,              # Max epochs (YOUR REQUIREMENT)\n",
    "    'n2v_steps_per_epoch': 50,          # Steps per epoch (more steps = more random masks per epoch)\n",
    "    'n2v_batch_size': 1,                # Batch size\n",
    "    'n2v_patch_size': 128,              # Full 128√ó128 image as patch (no cropping)\n",
    "    'learning_rate': 0.001,             # Adam lr = 1e-3 (YOUR REQUIREMENT)\n",
    "\n",
    "    # Multi-seed experiment (YOUR REQUIREMENTS)\n",
    "    'num_seeds': 5,                     # Number of independent runs\n",
    "    'base_seed': 42,                    # Starting seed for reproducibility\n",
    "\n",
    "    # Checkpoint settings\n",
    "    'save_every_epoch': True,           # Save checkpoint each epoch\n",
    "    'use_psnr_selection': True,         # Use PSNR (not loss) for best model\n",
    "\n",
    "    # ========== STAGE 2: PN2V Bootstrap (optional) ==========\n",
    "    'run_pn2v': False,                  # Set True to run PN2V after N2V\n",
    "    'n_gaussian': 3,\n",
    "    'n_coeff': 2,\n",
    "    'gmm_epochs': 2000,\n",
    "    'gmm_batch_size': 250000,           # 250k (paper default ‚Äî was 100k)\n",
    "    'pn2v_num_samples': 800,            # 800 samples (paper default ‚Äî was 100)\n",
    "                                        #   More samples ‚Üí better MMSE ‚Üí fewer artifacts\n",
    "    'pn2v_depth': 3,\n",
    "    'pn2v_start_filts': 64,             # 64 filters (paper default ‚Äî was 4)\n",
    "                                        #   Too few filters ‚Üí hallucinated structures\n",
    "    'pn2v_max_epochs': 200,             # 200 epochs (paper default ‚Äî was 150)\n",
    "    'pn2v_steps_per_epoch': 5,          # Steps per epoch (original: 5)\n",
    "    'pn2v_batch_size': 1,               # Batch size (original: 1)\n",
    "    'pn2v_patch_size': 64,              # Patch size for training\n",
    "    'pn2v_learning_rate': 0.001,\n",
    "    'pn2v_patience': 50,                # Early stopping patience\n",
    "}\n",
    "\n",
    "# Print configuration\n",
    "print(\"=\"*60)\n",
    "print(\"EXPERIMENT CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìÑ Data file: {CONFIG['data_file']}\")\n",
    "print(f\"üìÑ Ground truth: {CONFIG['ground_truth_file'] or 'Not provided (will use val loss)'}\")\n",
    "print(\"\\nüìå Network Architecture:\")\n",
    "print(f\"  U-Net depth: {CONFIG['net_depth']}\")\n",
    "print(f\"  Base channels (C): {CONFIG['net_start_filts']}\")\n",
    "print(f\"  Activation: ReLU\")\n",
    "print(f\"  Weight init: Kaiming (He)\")\n",
    "print(\"\\nüìå Masking:\")\n",
    "print(f\"  Mask patch: {CONFIG['mask_patch_size']}√ó{CONFIG['mask_patch_size']}\")\n",
    "print(f\"  Mask ratio: {CONFIG['mask_ratio']*100}%\")\n",
    "print(f\"  Replacement: Zero\")\n",
    "print(\"\\nüìå Training:\")\n",
    "print(f\"  Max epochs: {CONFIG['n2v_max_epochs']}\")\n",
    "print(f\"  Learning rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"  Batch size: {CONFIG['n2v_batch_size']}\")\n",
    "print(\"\\nüìå Experiment:\")\n",
    "print(f\"  Number of seeds: {CONFIG['num_seeds']}\")\n",
    "print(f\"  Base seed: {CONFIG['base_seed']}\")\n",
    "print(f\"  Per-epoch checkpoints: {CONFIG['save_every_epoch']}\")\n",
    "print(f\"  PSNR-based selection: {CONFIG['use_psnr_selection']}\")\n",
    "print(\"\\nüìå PN2V Bootstrap:\")\n",
    "print(f\"  Output samples: {CONFIG['pn2v_num_samples']}\")\n",
    "print(f\"  Start filters: {CONFIG['pn2v_start_filts']}\")\n",
    "print(f\"  GMM batch size: {CONFIG['gmm_batch_size']}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62297797",
   "metadata": {},
   "source": [
    "## üñºÔ∏è Section 4: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d28854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load noisy image (imports available from Section 5)\n",
    "data_path = os.path.join(DRIVE_DATA_PATH, CONFIG['data_file'])\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    print(f\"‚ö†Ô∏è  File not found: {data_path}\")\n",
    "    print(f\"\\nPlease upload your noisy image to:\")\n",
    "    print(f\"  {DRIVE_DATA_PATH}/{CONFIG['data_file']}\")\n",
    "    print(f\"\\nOr modify CONFIG['data_file'] to match your filename.\")\n",
    "else:\n",
    "    noisy_image = tifffile.imread(data_path).astype(np.float32)\n",
    "    \n",
    "    # Handle 3D stack - use first slice or choose\n",
    "    if noisy_image.ndim == 3:\n",
    "        print(f\"Loaded 3D stack: {noisy_image.shape}\")\n",
    "        print(f\"Using first slice for training...\")\n",
    "        noisy_image = noisy_image[0]  # Use first slice\n",
    "    \n",
    "    print(f\"\\n‚úì Loaded image: {noisy_image.shape}\")\n",
    "    print(f\"  Min: {noisy_image.min():.2f}\")\n",
    "    print(f\"  Max: {noisy_image.max():.2f}\")\n",
    "    print(f\"  Mean: {noisy_image.mean():.2f}\")\n",
    "    print(f\"  Std: {noisy_image.std():.2f}\")\n",
    "    \n",
    "    # Display\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(noisy_image, cmap='magma')\n",
    "    plt.colorbar()\n",
    "    plt.title('Noisy Input Image')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67557eeb",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Section 5: Setup & Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f0eb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ALL IMPORTS & SETUP\n",
    "# ============================================================\n",
    "# Consolidate all imports here to avoid dependency issues\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import init\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tifffile\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import copy\n",
    "import time as time_module\n",
    "\n",
    "# Add source to path (required for Colab)\n",
    "sys.path.insert(0, '/content/PPN2V/src')\n",
    "\n",
    "from ppn2v.unet.model import UNet\n",
    "from ppn2v.pn2v import training, utils, prediction\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# ============================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set all random seeds for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    print(f\"  Seed set to: {seed}\")\n",
    "\n",
    "def compute_psnr(gt, pred):\n",
    "    \"\"\"Compute PSNR between ground truth and prediction.\n",
    "\n",
    "    Uses the ground truth data range (max - min) as the standard reference.\n",
    "    This is the standard definition for microscopy images with arbitrary intensity ranges.\n",
    "    \"\"\"\n",
    "    mse = np.mean((gt.astype(np.float64) - pred.astype(np.float64)) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    data_range = float(gt.max() - gt.min())\n",
    "    if data_range == 0:\n",
    "        return float('inf')\n",
    "    psnr = 20 * np.log10(data_range / np.sqrt(mse))\n",
    "    return psnr\n",
    "\n",
    "def create_n2v_model(config, device):\n",
    "    \"\"\"Create and initialize N2V U-Net model.\"\"\"\n",
    "    net = UNet(\n",
    "        num_classes=1,                              # N2V: single output\n",
    "        in_channels=1,                              # Single channel input\n",
    "        depth=config['net_depth'],                  # 3\n",
    "        start_filts=config['net_start_filts'],      # 4 (YOUR REQUIREMENT)\n",
    "        merge_mode='add'\n",
    "    )\n",
    "\n",
    "    # Apply KAIMING (He) initialization for ReLU\n",
    "    def init_weights_kaiming(m):\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "            init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                init.constant_(m.bias, 0)\n",
    "\n",
    "    net.apply(init_weights_kaiming)\n",
    "    net = net.to(device)\n",
    "    return net\n",
    "\n",
    "def predict_and_compute_psnr(net, noisy_img, gt_img, device):\n",
    "    \"\"\"Run prediction and compute PSNR against ground truth.\"\"\"\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        # Ensure tile size is divisible by 2^depth\n",
    "        depth = net.depth if hasattr(net, 'depth') else 3\n",
    "        divisor = 2 ** depth\n",
    "        raw_ps = min(128, noisy_img.shape[0], noisy_img.shape[1])\n",
    "        ps = (raw_ps // divisor) * divisor\n",
    "        if ps < divisor:\n",
    "            ps = divisor\n",
    "\n",
    "        # tiledPredict with noiseModel=None returns a single array (not a tuple)\n",
    "        denoised = prediction.tiledPredict(\n",
    "            im=noisy_img.astype(np.float32),\n",
    "            net=net,\n",
    "            ps=ps,\n",
    "            overlap=32,\n",
    "            noiseModel=None,\n",
    "            device=device\n",
    "        )\n",
    "    psnr = compute_psnr(gt_img, denoised)\n",
    "    return denoised, psnr\n",
    "\n",
    "# Print model info\n",
    "test_net = create_n2v_model(CONFIG, device)\n",
    "n_params = sum(p.numel() for p in test_net.parameters())\n",
    "print(f\"\\nN2V Network Configuration:\")\n",
    "print(f\"  Depth: {CONFIG['net_depth']}\")\n",
    "print(f\"  Base channels (C): {CONFIG['net_start_filts']}\")\n",
    "print(f\"  Activation: ReLU\")\n",
    "print(f\"  Weight init: Kaiming (He)\")\n",
    "print(f\"  Parameters: {n_params:,}\")\n",
    "del test_net\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n‚úì All imports and setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48500b86",
   "metadata": {},
   "source": [
    "## üöÄ Section 6: Multi-Seed Training with Per-Epoch Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996421aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MULTI-SEED N2V TRAINING WITH PER-EPOCH CHECKPOINTS\n",
    "# ============================================================\n",
    "# Requirements met:\n",
    "# - 128√ó128√ó1 image, full image as patch (no cropping)\n",
    "# - mask_patch=7 (7√ó7), mask_ratio=0.1 (10%), zero replacement\n",
    "# - MSE loss ONLY on masked pixels\n",
    "# - U-Net: C=4 base channels, depth=3, ReLU, Kaiming init\n",
    "# - lr=1e-3 with ReduceLROnPlateau scheduler, up to 150 epochs\n",
    "# - Per-epoch checkpoints, PSNR-based best model selection\n",
    "# - N independent seed runs with logging\n",
    "# - augment=False (single image, no augmentations)\n",
    "# ============================================================\n",
    "# NOTE: All imports (torch.optim, time, etc.) available from Section 5\n",
    "\n",
    "# Load ground truth if available (for PSNR-based selection)\n",
    "ground_truth = None\n",
    "if CONFIG['ground_truth_file']:\n",
    "    gt_path = os.path.join(DRIVE_DATA_PATH, CONFIG['ground_truth_file'])\n",
    "    if os.path.exists(gt_path):\n",
    "        ground_truth = tifffile.imread(gt_path).astype(np.float32)\n",
    "        if ground_truth.ndim == 3:\n",
    "            ground_truth = ground_truth[0]\n",
    "        print(f\"‚úì Ground truth loaded: {ground_truth.shape}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Ground truth not found: {gt_path}\")\n",
    "\n",
    "# Prepare data\n",
    "if noisy_image.ndim == 2:\n",
    "    train_data = noisy_image[np.newaxis, :, :].copy()\n",
    "    val_data = noisy_image[np.newaxis, :, :].copy()\n",
    "else:\n",
    "    train_data = noisy_image[:-5].copy()\n",
    "    val_data = noisy_image[-5:].copy()\n",
    "\n",
    "print(f\"\\nüìä Data Shapes:\")\n",
    "print(f\"  Noisy image: {noisy_image.shape}\")\n",
    "print(f\"  Train data: {train_data.shape}\")\n",
    "print(f\"  Val data: {val_data.shape}\")\n",
    "\n",
    "# Calculate parameters ‚Äî use full image as patch for single 128√ó128 inputs\n",
    "H, W = train_data.shape[1], train_data.shape[2]\n",
    "patch_size = CONFIG['n2v_patch_size']  # 128 ‚Äî full image, no cropping\n",
    "if patch_size > min(H, W):\n",
    "    patch_size = min(H, W)\n",
    "    print(f\"‚ö†Ô∏è Patch size clamped to image size: {patch_size}\")\n",
    "num_masked_pixels = int(patch_size * patch_size * CONFIG['mask_ratio'])\n",
    "\n",
    "print(f\"\\nüìå Training Parameters:\")\n",
    "print(f\"  Patch size: {patch_size}√ó{patch_size}\")\n",
    "print(f\"  Masked pixels per patch: {num_masked_pixels} ({CONFIG['mask_ratio']*100}%)\")\n",
    "print(f\"  Mask patch size: {CONFIG['mask_patch_size']}√ó{CONFIG['mask_patch_size']}\")\n",
    "print(f\"  Max epochs: {CONFIG['n2v_max_epochs']}\")\n",
    "print(f\"  Steps per epoch: {CONFIG['n2v_steps_per_epoch']}\")\n",
    "print(f\"  Number of seeds: {CONFIG['num_seeds']}\")\n",
    "\n",
    "# ============================================================\n",
    "# CUSTOM TRAINING LOOP WITH PER-EPOCH CHECKPOINTS & PSNR\n",
    "# ============================================================\n",
    "\n",
    "def train_single_seed(seed, config, train_data, val_data, noisy_img, gt_img, device, results_path):\n",
    "    \"\"\"Train N2V for a single seed with per-epoch checkpoints and PSNR tracking.\"\"\"\n",
    "\n",
    "    set_seed(seed)\n",
    "    seed_dir = os.path.join(results_path, f'seed_{seed}')\n",
    "    os.makedirs(seed_dir, exist_ok=True)\n",
    "\n",
    "    # Create fresh model\n",
    "    net = create_n2v_model(config, device)\n",
    "\n",
    "    # Compute normalization from data (matches original trainNetwork)\n",
    "    combined = np.concatenate((train_data, val_data))\n",
    "    net.mean = np.mean(combined)\n",
    "    net.std = np.std(combined)\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=config['learning_rate'])\n",
    "    # LR scheduler ‚Äî matches original trainNetwork: halve LR when val loss plateaus\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5)\n",
    "\n",
    "    # Training tracking\n",
    "    train_hist = []\n",
    "    val_hist = []\n",
    "    psnr_hist = []\n",
    "    best_psnr = -float('inf')\n",
    "    best_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "\n",
    "    patch_size = config['n2v_patch_size']  # 128 ‚Äî full image as patch\n",
    "    if patch_size > min(train_data.shape[1], train_data.shape[2]):\n",
    "        patch_size = min(train_data.shape[1], train_data.shape[2])\n",
    "    num_pix = int(patch_size * patch_size * config['mask_ratio'])\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SEED {seed} - Starting Training\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    start_time = time_module.time()\n",
    "\n",
    "    for epoch in range(config['n2v_max_epochs']):\n",
    "        # ---- TRAINING ----\n",
    "        net.train()\n",
    "        epoch_losses = []\n",
    "        data_counter = 0\n",
    "\n",
    "        for step in range(config['n2v_steps_per_epoch']):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Virtual batch (accumulate gradients over 20 forward passes)\n",
    "            for _ in range(20):  # virtualBatchSize=20\n",
    "                outputs, labels, masks, data_counter = training.trainingPred(\n",
    "                    train_data, net, data_counter, patch_size,\n",
    "                    config['n2v_batch_size'], num_pix, device,\n",
    "                    augment=False, supervised=False,\n",
    "                    maskPatchSize=config['mask_patch_size'],\n",
    "                    useZeroReplacement=True\n",
    "                )\n",
    "                loss = training.lossFunctionN2V(outputs, labels, masks) / (net.std ** 2)\n",
    "                loss.backward()\n",
    "                epoch_losses.append(loss.item())\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_train_loss = np.mean(epoch_losses)\n",
    "        train_hist.append(avg_train_loss)\n",
    "\n",
    "        # ---- VALIDATION ----\n",
    "        net.eval()\n",
    "        val_losses = []\n",
    "        val_counter = 0\n",
    "        with torch.no_grad():\n",
    "            for _ in range(20):  # valSize=20\n",
    "                outputs, labels, masks, val_counter = training.trainingPred(\n",
    "                    val_data, net, val_counter, patch_size,\n",
    "                    config['n2v_batch_size'], num_pix, device,\n",
    "                    augment=False, supervised=False,\n",
    "                    maskPatchSize=config['mask_patch_size'],\n",
    "                    useZeroReplacement=True\n",
    "                )\n",
    "                loss = training.lossFunctionN2V(outputs, labels, masks) / (net.std ** 2)\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "        val_hist.append(avg_val_loss)\n",
    "\n",
    "        # Step the LR scheduler based on validation loss\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        # Compute PSNR if ground truth available\n",
    "        current_psnr = None\n",
    "        if gt_img is not None:\n",
    "            _, current_psnr = predict_and_compute_psnr(net, noisy_img, gt_img, device)\n",
    "            psnr_hist.append(current_psnr)\n",
    "\n",
    "        # Save checkpoint EVERY epoch\n",
    "        if config['save_every_epoch']:\n",
    "            checkpoint_path = os.path.join(seed_dir, f'epoch_{epoch+1:03d}.net')\n",
    "            torch.save(net, checkpoint_path)\n",
    "\n",
    "        # Determine best model\n",
    "        is_best = False\n",
    "        if config['use_psnr_selection'] and current_psnr is not None:\n",
    "            # Use PSNR for selection (higher is better)\n",
    "            if current_psnr > best_psnr:\n",
    "                best_psnr = current_psnr\n",
    "                best_epoch = epoch + 1\n",
    "                is_best = True\n",
    "        else:\n",
    "            # Use validation loss (lower is better)\n",
    "            if avg_val_loss < best_loss:\n",
    "                best_loss = avg_val_loss\n",
    "                best_epoch = epoch + 1\n",
    "                is_best = True\n",
    "\n",
    "        if is_best:\n",
    "            torch.save(net, os.path.join(seed_dir, 'best.net'))\n",
    "\n",
    "        # Log progress every 10 epochs\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            psnr_str = f\", PSNR: {current_psnr:.2f} dB\" if current_psnr else \"\"\n",
    "            best_str = f\" ‚úì BEST\" if is_best else \"\"\n",
    "            lr_str = f\", lr={current_lr:.1e}\" if current_lr != config['learning_rate'] else \"\"\n",
    "            print(f\"  Epoch {epoch+1:3d}/{config['n2v_max_epochs']}: \"\n",
    "                  f\"Train={avg_train_loss:.6f}, Val={avg_val_loss:.6f}{psnr_str}{lr_str}{best_str}\")\n",
    "\n",
    "    # Save final model\n",
    "    torch.save(net, os.path.join(seed_dir, 'last.net'))\n",
    "\n",
    "    # Save histories\n",
    "    np.save(os.path.join(seed_dir, 'train_hist.npy'), train_hist)\n",
    "    np.save(os.path.join(seed_dir, 'val_hist.npy'), val_hist)\n",
    "    if psnr_hist:\n",
    "        np.save(os.path.join(seed_dir, 'psnr_hist.npy'), psnr_hist)\n",
    "\n",
    "    elapsed = (time_module.time() - start_time) / 60\n",
    "\n",
    "    # Results\n",
    "    result = {\n",
    "        'seed': seed,\n",
    "        'best_epoch': best_epoch,\n",
    "        'best_psnr': best_psnr if psnr_hist else None,\n",
    "        'best_val_loss': min(val_hist),\n",
    "        'final_train_loss': train_hist[-1],\n",
    "        'final_val_loss': val_hist[-1],\n",
    "        'train_hist': train_hist,\n",
    "        'val_hist': val_hist,\n",
    "        'psnr_hist': psnr_hist if psnr_hist else None,\n",
    "        'elapsed_min': elapsed\n",
    "    }\n",
    "\n",
    "    print(f\"\\n  ‚úì Seed {seed} complete in {elapsed:.1f} min\")\n",
    "    print(f\"    Best epoch: {best_epoch}\")\n",
    "    if best_psnr > -float('inf'):\n",
    "        print(f\"    Best PSNR: {best_psnr:.2f} dB\")\n",
    "    print(f\"    Best val loss: {min(val_hist):.6f}\")\n",
    "\n",
    "    # Cleanup\n",
    "    del net, optimizer, scheduler\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return result\n",
    "\n",
    "# ============================================================\n",
    "# RUN ALL SEEDS\n",
    "# ============================================================\n",
    "\n",
    "all_results = []\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"STARTING {CONFIG['num_seeds']}-SEED EXPERIMENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i in range(CONFIG['num_seeds']):\n",
    "    seed = CONFIG['base_seed'] + i\n",
    "    result = train_single_seed(\n",
    "        seed=seed,\n",
    "        config=CONFIG,\n",
    "        train_data=train_data,\n",
    "        val_data=val_data,\n",
    "        noisy_img=noisy_image,\n",
    "        gt_img=ground_truth,\n",
    "        device=device,\n",
    "        results_path=N2V_RESULTS_PATH\n",
    "    )\n",
    "    all_results.append(result)\n",
    "\n",
    "# ============================================================\n",
    "# SUMMARY TABLE\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n{'Seed':<8} {'Best Epoch':<12} {'Best PSNR':<12} {'Best Val Loss':<15}\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "psnr_values = []\n",
    "for r in all_results:\n",
    "    psnr_str = f\"{r['best_psnr']:.2f} dB\" if r['best_psnr'] else \"N/A\"\n",
    "    print(f\"{r['seed']:<8} {r['best_epoch']:<12} {psnr_str:<12} {r['best_val_loss']:.6f}\")\n",
    "    if r['best_psnr']:\n",
    "        psnr_values.append(r['best_psnr'])\n",
    "\n",
    "if psnr_values:\n",
    "    print(\"-\"*50)\n",
    "    print(f\"{'Mean':<8} {'':<12} {np.mean(psnr_values):.2f} dB\")\n",
    "    print(f\"{'Std':<8} {'':<12} {np.std(psnr_values):.2f} dB\")\n",
    "\n",
    "# Save summary\n",
    "summary = {\n",
    "    'config': CONFIG,\n",
    "    'results': all_results,\n",
    "    'mean_psnr': np.mean(psnr_values) if psnr_values else None,\n",
    "    'std_psnr': np.std(psnr_values) if psnr_values else None\n",
    "}\n",
    "np.save(os.path.join(N2V_RESULTS_PATH, 'experiment_summary.npy'), summary, allow_pickle=True)\n",
    "\n",
    "print(\"\\n‚úì Experiment complete!\")\n",
    "print(f\"  Results saved to: {N2V_RESULTS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cf0e8c",
   "metadata": {},
   "source": [
    "## üîÆ Section 7: Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6c3f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ppn2v.pn2v import prediction\n",
    "\n",
    "# ============================================================\n",
    "# LOAD BEST MODEL FROM MULTI-SEED EXPERIMENT\n",
    "# ============================================================\n",
    "# We select the seed with highest PSNR (or lowest val loss if no GT)\n",
    "\n",
    "if all_results:\n",
    "    # Find best seed\n",
    "    if all_results[0]['best_psnr'] is not None:\n",
    "        # Use PSNR criterion\n",
    "        best_result = max(all_results, key=lambda x: x['best_psnr'])\n",
    "        print(f\"Selected best seed by PSNR: seed {best_result['seed']} ({best_result['best_psnr']:.2f} dB)\")\n",
    "    else:\n",
    "        # Use validation loss criterion\n",
    "        best_result = min(all_results, key=lambda x: x['best_val_loss'])\n",
    "        print(f\"Selected best seed by val loss: seed {best_result['seed']} ({best_result['best_val_loss']:.6f})\")\n",
    "\n",
    "    best_seed = best_result['seed']\n",
    "else:\n",
    "    # Fallback: use base seed\n",
    "    best_seed = CONFIG['base_seed']\n",
    "    print(f\"Using default seed: {best_seed}\")\n",
    "\n",
    "# Load best model from selected seed\n",
    "seed_dir = os.path.join(N2V_RESULTS_PATH, f'seed_{best_seed}')\n",
    "best_model_path = os.path.join(seed_dir, 'best.net')\n",
    "\n",
    "n2v_net_loaded = torch.load(best_model_path, map_location=device, weights_only=False)\n",
    "n2v_net_loaded = n2v_net_loaded.to(device)\n",
    "n2v_net_loaded.eval()\n",
    "\n",
    "print(f\"\\n‚úì Loaded best model: {best_model_path}\")\n",
    "print(f\"  From seed {best_seed}, epoch {best_result['best_epoch']}\")\n",
    "print(f\"  Mean: {n2v_net_loaded.mean:.2f}\")\n",
    "print(f\"  Std: {n2v_net_loaded.std:.2f}\")\n",
    "\n",
    "# Prepare image for prediction (ensure 2D)\n",
    "noisy_for_pred = np.squeeze(noisy_image).astype(np.float32)\n",
    "\n",
    "# Ensure tile size is divisible by 2^depth\n",
    "depth = CONFIG['net_depth']\n",
    "divisor = 2 ** depth\n",
    "raw_ps = min(256, noisy_for_pred.shape[0], noisy_for_pred.shape[1])\n",
    "ps = (raw_ps // divisor) * divisor\n",
    "if ps < divisor:\n",
    "    ps = divisor\n",
    "\n",
    "# IMPORTANT: For N2V (noiseModel=None), tiledPredict returns a single array\n",
    "n2v_denoised = prediction.tiledPredict(\n",
    "    im=noisy_for_pred,\n",
    "    net=n2v_net_loaded,\n",
    "    ps=ps,               # Tile size (divisible by 2^depth)\n",
    "    overlap=48,           # Overlap between tiles\n",
    "    noiseModel=None,      # None for standard N2V\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì N2V Prediction complete!\")\n",
    "print(f\"  Denoised image shape: {n2v_denoised.shape}\")\n",
    "print(f\"  Min: {n2v_denoised.min():.2f}\")\n",
    "print(f\"  Max: {n2v_denoised.max():.2f}\")\n",
    "\n",
    "# Final PSNR if ground truth available\n",
    "if ground_truth is not None:\n",
    "    final_psnr = compute_psnr(ground_truth, n2v_denoised)\n",
    "    print(f\"  Final PSNR: {final_psnr:.2f} dB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5078f4c5",
   "metadata": {},
   "source": [
    "## üìä Section 8: N2V Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912a0634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MULTI-SEED EXPERIMENT VISUALIZATION\n",
    "# ============================================================\n",
    "# NOTE: All imports (pd, plt, np, etc.) available from Section 5\n",
    "\n",
    "# 1. N2V Side-by-side comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Noisy input\n",
    "im0 = axes[0].imshow(noisy_image, cmap='magma')\n",
    "axes[0].set_title('Noisy Input', fontsize=14)\n",
    "axes[0].axis('off')\n",
    "plt.colorbar(im0, ax=axes[0], fraction=0.046)\n",
    "\n",
    "# N2V Denoised output\n",
    "im1 = axes[1].imshow(n2v_denoised, cmap='magma')\n",
    "title_psnr = f\" ({compute_psnr(ground_truth, n2v_denoised):.2f} dB)\" if ground_truth is not None else \"\"\n",
    "axes[1].set_title(f'N2V Denoised{title_psnr}', fontsize=14)\n",
    "axes[1].axis('off')\n",
    "plt.colorbar(im1, ax=axes[1], fraction=0.046)\n",
    "\n",
    "# Difference (noise removed)\n",
    "n2v_difference = noisy_image - n2v_denoised\n",
    "vmax = np.abs(n2v_difference).max()\n",
    "im2 = axes[2].imshow(n2v_difference, cmap='RdBu_r', vmin=-vmax, vmax=vmax)\n",
    "axes[2].set_title('N2V Removed Noise', fontsize=14)\n",
    "axes[2].axis('off')\n",
    "plt.colorbar(im2, ax=axes[2], fraction=0.046)\n",
    "\n",
    "plt.suptitle(f'N2V Results (Best Seed: {best_seed})', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(N2V_RESULTS_PATH, 'n2v_comparison.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# 2. COMPUTE AVERAGES ACROSS SEEDS\n",
    "# ============================================================\n",
    "n_epochs = len(all_results[0]['val_hist'])\n",
    "epochs = np.arange(1, n_epochs + 1)\n",
    "\n",
    "# Stack all histories into arrays [n_seeds, n_epochs]\n",
    "val_loss_all = np.array([r['val_hist'] for r in all_results])\n",
    "train_loss_all = np.array([r['train_hist'] for r in all_results])\n",
    "\n",
    "# Compute mean and std\n",
    "val_loss_mean = np.mean(val_loss_all, axis=0)\n",
    "val_loss_std = np.std(val_loss_all, axis=0)\n",
    "train_loss_mean = np.mean(train_loss_all, axis=0)\n",
    "train_loss_std = np.std(train_loss_all, axis=0)\n",
    "\n",
    "# PSNR averages (if available)\n",
    "has_psnr = all_results[0]['psnr_hist'] is not None\n",
    "if has_psnr:\n",
    "    psnr_all = np.array([r['psnr_hist'] for r in all_results])\n",
    "    psnr_mean = np.mean(psnr_all, axis=0)\n",
    "    psnr_std = np.std(psnr_all, axis=0)\n",
    "\n",
    "    # Find best average PSNR and its epoch\n",
    "    best_avg_psnr_idx = np.argmax(psnr_mean)\n",
    "    best_avg_psnr_epoch = best_avg_psnr_idx + 1  # 1-indexed\n",
    "    best_avg_psnr_value = psnr_mean[best_avg_psnr_idx]\n",
    "\n",
    "    # Noisy baseline PSNR\n",
    "    noisy_psnr = compute_psnr(ground_truth, noisy_image)\n",
    "\n",
    "    print(f\"üìä Best Average PSNR: {best_avg_psnr_value:.2f} dB @ epoch {best_avg_psnr_epoch}\")\n",
    "    print(f\"üìä Noisy Baseline PSNR: {noisy_psnr:.2f} dB\")\n",
    "\n",
    "# ============================================================\n",
    "# 3. SAVE AVERAGES TO CSV\n",
    "# ============================================================\n",
    "csv_data = {\n",
    "    'epoch': epochs,\n",
    "    'train_loss_mean': train_loss_mean,\n",
    "    'train_loss_std': train_loss_std,\n",
    "    'val_loss_mean': val_loss_mean,\n",
    "    'val_loss_std': val_loss_std,\n",
    "}\n",
    "if has_psnr:\n",
    "    csv_data['psnr_mean'] = psnr_mean\n",
    "    csv_data['psnr_std'] = psnr_std\n",
    "\n",
    "df_averages = pd.DataFrame(csv_data)\n",
    "csv_path = os.path.join(N2V_RESULTS_PATH, 'epoch_averages.csv')\n",
    "df_averages.to_csv(csv_path, index=False)\n",
    "print(f\"‚úì Per-epoch averages saved to: {csv_path}\")\n",
    "\n",
    "# ============================================================\n",
    "# 4. FIGURE 1: AVERAGED LOSS vs EPOCHS\n",
    "# ============================================================\n",
    "if has_psnr:\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "    # Validation loss: blue line with shaded ¬±1 std\n",
    "    ax.plot(epochs, val_loss_mean, color='tab:blue', lw=1.8, label='Validation Loss (mean)')\n",
    "    ax.fill_between(epochs,\n",
    "                    val_loss_mean - val_loss_std,\n",
    "                    val_loss_mean + val_loss_std,\n",
    "                    color='tab:blue', alpha=0.18)\n",
    "\n",
    "    # Training loss: orange line with shaded ¬±1 std\n",
    "    ax.plot(epochs, train_loss_mean, color='tab:orange', lw=1.8, label='Training Loss (mean)')\n",
    "    ax.fill_between(epochs,\n",
    "                    train_loss_mean - train_loss_std,\n",
    "                    train_loss_mean + train_loss_std,\n",
    "                    color='tab:orange', alpha=0.18)\n",
    "\n",
    "    # Vertical dashed red line at best-PSNR epoch\n",
    "    ax.axvline(best_avg_psnr_epoch, color='red', linestyle='--', lw=1.5, alpha=0.85,\n",
    "               label=f'Best PSNR epoch = {best_avg_psnr_epoch}')\n",
    "\n",
    "    ax.set_xlabel('Epoch', fontsize=12)\n",
    "    ax.set_ylabel('Loss', fontsize=12)\n",
    "    ax.set_title(f'Averaged Loss vs Epochs (N={CONFIG[\"num_seeds\"]} seeds)',\n",
    "                 fontsize=13, fontweight='bold')\n",
    "    ax.legend(loc='upper right', fontsize=9, framealpha=0.9)\n",
    "    ax.set_xlim(1, n_epochs)\n",
    "    ax.grid(True, alpha=0.25, linestyle='-')\n",
    "    ax.tick_params(labelsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    loss_plot_path = os.path.join(N2V_RESULTS_PATH, 'avg_loss_vs_epochs.png')\n",
    "    plt.savefig(loss_plot_path, dpi=200, bbox_inches='tight')\n",
    "    print(f\"‚úì Loss plot saved: {loss_plot_path}\")\n",
    "    plt.show()\n",
    "\n",
    "    # ============================================================\n",
    "    # 5. FIGURE 2: AVERAGED PSNR vs EPOCHS\n",
    "    # ============================================================\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "    # PSNR curve: blue line with shaded ¬±1 std\n",
    "    ax.plot(epochs, psnr_mean, color='tab:blue', lw=1.8, label='PSNR (mean)')\n",
    "    ax.fill_between(epochs,\n",
    "                    psnr_mean - psnr_std,\n",
    "                    psnr_mean + psnr_std,\n",
    "                    color='tab:blue', alpha=0.18)\n",
    "\n",
    "    # Red \"X\" marker at peak\n",
    "    ax.plot(best_avg_psnr_epoch, best_avg_psnr_value, 'rX', markersize=14,\n",
    "            markeredgewidth=2.0, zorder=5)\n",
    "\n",
    "    # Annotation with arrow pointing to peak\n",
    "    offset_x = n_epochs * 0.12\n",
    "    offset_y = (psnr_mean.max() - psnr_mean.min()) * 0.18\n",
    "    ax.annotate(\n",
    "        f'Best PSNR: {best_avg_psnr_value:.2f} dB\\nepoch = {best_avg_psnr_epoch}',\n",
    "        xy=(best_avg_psnr_epoch, best_avg_psnr_value),\n",
    "        xytext=(best_avg_psnr_epoch + offset_x, best_avg_psnr_value - offset_y),\n",
    "        fontsize=10, fontweight='bold', color='red',\n",
    "        arrowprops=dict(arrowstyle='->', color='red', lw=1.5),\n",
    "        bbox=dict(boxstyle='round,pad=0.3', facecolor='white', edgecolor='red', alpha=0.9)\n",
    "    )\n",
    "\n",
    "    # Horizontal dashed red line at noisy-image baseline PSNR\n",
    "    ax.axhline(noisy_psnr, color='red', linestyle='--', lw=1.2, alpha=0.7,\n",
    "               label=f'Noisy baseline = {noisy_psnr:.2f} dB')\n",
    "\n",
    "    ax.set_xlabel('Epoch', fontsize=12)\n",
    "    ax.set_ylabel('PSNR (dB)', fontsize=12)\n",
    "    ax.set_title(f'Averaged PSNR vs Epochs (N={CONFIG[\"num_seeds\"]} seeds)',\n",
    "                 fontsize=13, fontweight='bold')\n",
    "    ax.legend(loc='lower right', fontsize=9, framealpha=0.9)\n",
    "    ax.set_xlim(1, n_epochs)\n",
    "    ax.grid(True, alpha=0.25, linestyle='-')\n",
    "    ax.tick_params(labelsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    psnr_plot_path = os.path.join(N2V_RESULTS_PATH, 'avg_psnr_vs_epochs.png')\n",
    "    plt.savefig(psnr_plot_path, dpi=200, bbox_inches='tight')\n",
    "    print(f\"‚úì PSNR plot saved: {psnr_plot_path}\")\n",
    "    plt.show()\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"EXPERIMENT SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Number of seeds: {CONFIG['num_seeds']}\")\n",
    "    print(f\"Best avg PSNR: {best_avg_psnr_value:.2f} ¬± {psnr_std[best_avg_psnr_idx]:.2f} dB  @ epoch {best_avg_psnr_epoch}\")\n",
    "    print(f\"Noisy baseline: {noisy_psnr:.2f} dB\")\n",
    "    psnr_vals = [r['best_psnr'] for r in all_results]\n",
    "    print(f\"Best individual PSNR: {max(psnr_vals):.2f} dB (seed {best_seed})\")\n",
    "\n",
    "else:\n",
    "    # Fallback: loss-only plot when no ground truth\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.plot(epochs, val_loss_mean, color='tab:blue', lw=1.8, label='Validation Loss (mean)')\n",
    "    ax.fill_between(epochs, val_loss_mean - val_loss_std, val_loss_mean + val_loss_std,\n",
    "                    color='tab:blue', alpha=0.18)\n",
    "    ax.plot(epochs, train_loss_mean, color='tab:orange', lw=1.8, label='Training Loss (mean)')\n",
    "    ax.fill_between(epochs, train_loss_mean - train_loss_std, train_loss_mean + train_loss_std,\n",
    "                    color='tab:orange', alpha=0.18)\n",
    "    ax.set_xlabel('Epoch', fontsize=12)\n",
    "    ax.set_ylabel('Loss', fontsize=12)\n",
    "    ax.set_title(f'Averaged Loss vs Epochs (N={CONFIG[\"num_seeds\"]} seeds)', fontsize=13, fontweight='bold')\n",
    "    ax.legend(loc='upper right', fontsize=9)\n",
    "    ax.set_xlim(1, n_epochs)\n",
    "    ax.grid(True, alpha=0.25)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(N2V_RESULTS_PATH, 'avg_loss_vs_epochs.png'), dpi=200, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"\\n‚ö†Ô∏è No ground truth provided ‚Äî PSNR plot skipped\")\n",
    "    print(\"Set CONFIG['ground_truth_file'] to enable PSNR tracking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6c372a",
   "metadata": {},
   "source": [
    "## üíæ Section 9: Save N2V Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebce8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save N2V denoised image\n",
    "n2v_output_filename = f\"n2v_denoised_{CONFIG['data_name']}.tif\"\n",
    "n2v_output_path = os.path.join(N2V_RESULTS_PATH, n2v_output_filename)\n",
    "tifffile.imwrite(n2v_output_path, n2v_denoised.astype(np.float32))\n",
    "print(f\"‚úì N2V denoised image saved: {n2v_output_path}\")\n",
    "\n",
    "# Compute difference locally (independent of Section 8 plots)\n",
    "n2v_difference = noisy_image - n2v_denoised\n",
    "\n",
    "# Save N2V difference image\n",
    "n2v_diff_filename = f\"n2v_noise_removed_{CONFIG['data_name']}.tif\"\n",
    "n2v_diff_path = os.path.join(N2V_RESULTS_PATH, n2v_diff_filename)\n",
    "tifffile.imwrite(n2v_diff_path, n2v_difference.astype(np.float32))\n",
    "print(f\"‚úì N2V noise map saved: {n2v_diff_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ STAGE 1 COMPLETE: N2V Results Saved\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Results directory: {N2V_RESULTS_PATH}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fde34d1",
   "metadata": {},
   "source": [
    "---\n",
    "# üî¨ STAGE 2: PN2V Bootstrap\n",
    "\n",
    "Now we use the N2V prediction as the \"signal estimate\" to build a noise model, then train PN2V for probabilistic denoising with uncertainty quantification.\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Section 10: Create GMM Noise Model (Bootstrap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba25884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üßπ CRITICAL: Clear GPU memory before Stage 2\n",
    "# ============================================================\n",
    "import gc\n",
    "\n",
    "# Delete N2V model to free GPU memory\n",
    "# Note: Only n2v_net_loaded exists (from Section 7), not n2v_net\n",
    "try:\n",
    "    del n2v_net_loaded\n",
    "    print(\"‚úì Deleted n2v_net_loaded\")\n",
    "except NameError:\n",
    "    print(\"‚ö†Ô∏è n2v_net_loaded already deleted or not created\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\nGPU Memory after cleanup:\")\n",
    "print(f\"  Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "print(f\"  Cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "\n",
    "# ============================================================\n",
    "# VERIFY REQUIRED VARIABLES FROM STAGE 1\n",
    "# ============================================================\n",
    "if 'n2v_denoised' not in dir():\n",
    "    # Try to load from saved file\n",
    "    n2v_saved_path = os.path.join(N2V_RESULTS_PATH, f\"n2v_denoised_{CONFIG['data_name']}.tif\")\n",
    "    if os.path.exists(n2v_saved_path):\n",
    "        print(f\"\\nüìÇ Loading N2V result from: {n2v_saved_path}\")\n",
    "        n2v_denoised = tifffile.imread(n2v_saved_path).astype(np.float32)\n",
    "    else:\n",
    "        raise RuntimeError(\n",
    "            f\"n2v_denoised not found! Please run Stage 1 (Sections 6-9) first,\\n\"\n",
    "            f\"or place the N2V denoised image at: {n2v_saved_path}\"\n",
    "        )\n",
    "\n",
    "if 'noisy_image' not in dir():\n",
    "    raise RuntimeError(\"noisy_image not found! Please run Section 4 first.\")\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "from ppn2v.pn2v import gaussianMixtureNoiseModel\n",
    "\n",
    "# Bootstrap: Use N2V prediction as signal estimate\n",
    "# observation = noisy image, signal = N2V denoised\n",
    "# Use 2D versions and ensure float32\n",
    "noisy_flat = np.squeeze(noisy_image).astype(np.float32).flatten()\n",
    "signal_flat = n2v_denoised.astype(np.float32).flatten()\n",
    "\n",
    "# Determine signal range for noise model\n",
    "min_signal = float(np.percentile(signal_flat, 0.5))\n",
    "max_signal = float(np.percentile(signal_flat, 99.5))\n",
    "\n",
    "print(\"\\nCreating GMM Noise Model (Bootstrap from N2V)...\")\n",
    "print(f\"  Signal range: [{min_signal:.2f}, {max_signal:.2f}]\")\n",
    "print(f\"  Gaussians: {CONFIG['n_gaussian']}\")\n",
    "print(f\"  Coefficients: {CONFIG['n_coeff']}\")\n",
    "\n",
    "# Create and train GMM noise model\n",
    "gmm_noise_model = gaussianMixtureNoiseModel.GaussianMixtureNoiseModel(\n",
    "    min_signal=min_signal,\n",
    "    max_signal=max_signal,\n",
    "    path=PN2V_RESULTS_PATH,\n",
    "    weight=None,\n",
    "    n_gaussian=CONFIG['n_gaussian'],\n",
    "    n_coeff=CONFIG['n_coeff'],\n",
    "    device=device,\n",
    "    min_sigma=50  # Prevents degenerate solutions\n",
    ")\n",
    "\n",
    "# Train the noise model\n",
    "gmm_noise_model.train(\n",
    "    signal=signal_flat,\n",
    "    observation=noisy_flat,\n",
    "    batchSize=CONFIG['gmm_batch_size'],\n",
    "    n_epochs=CONFIG['gmm_epochs'],\n",
    "    learning_rate=0.1,\n",
    "    name=f\"GMMNoiseModel_{CONFIG['data_name']}_bootstrap\"\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì GMM Noise Model trained and saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b804798f",
   "metadata": {},
   "source": [
    "## üèãÔ∏è Section 11: Train PN2V Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c20870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: UNet, training, utils already imported in Section 5\n",
    "\n",
    "# Print memory status before creating PN2V network\n",
    "print(f\"GPU Memory before PN2V:\")\n",
    "print(f\"  Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "print(f\"  Free: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()) / 1e9:.2f} GB\")\n",
    "\n",
    "# Create PN2V U-Net\n",
    "# Original MouseActin: UNet(800, depth=3) with default 64 start_filts\n",
    "pn2v_net = UNet(\n",
    "    num_classes=CONFIG['pn2v_num_samples'],  # 800 samples for accurate MMSE\n",
    "    in_channels=1,\n",
    "    depth=CONFIG['pn2v_depth'],              # 3 (matches original)\n",
    "    start_filts=CONFIG['pn2v_start_filts'],  # 64 (matches original ‚Äî prevents hallucination)\n",
    "    merge_mode='add'\n",
    ")\n",
    "pn2v_net = pn2v_net.to(device)\n",
    "\n",
    "# Prepare training data - same structure as original\n",
    "# Original: my_train_data=data[:-5].copy(), my_val_data=data[-5:].copy()\n",
    "if noisy_image.ndim == 2:\n",
    "    pn2v_train_data = noisy_image[np.newaxis, :, :]\n",
    "    pn2v_val_data = noisy_image[np.newaxis, :, :]\n",
    "else:\n",
    "    pn2v_train_data = noisy_image[:-5].copy()\n",
    "    pn2v_val_data = noisy_image[-5:].copy()\n",
    "    np.random.shuffle(pn2v_train_data)  # Original shuffles\n",
    "    np.random.shuffle(pn2v_val_data)\n",
    "\n",
    "# Count parameters\n",
    "n_params = sum(p.numel() for p in pn2v_net.parameters())\n",
    "\n",
    "print(f\"\\nPN2V Network created:\")\n",
    "print(f\"  Output samples: {CONFIG['pn2v_num_samples']}\")\n",
    "print(f\"  Depth: {CONFIG['pn2v_depth']}\")\n",
    "print(f\"  Start filters: {CONFIG['pn2v_start_filts']}\")\n",
    "print(f\"  Parameters: {n_params:,}\")\n",
    "print(f\"  Train data: {pn2v_train_data.shape}\")\n",
    "print(f\"  Val data: {pn2v_val_data.shape}\")\n",
    "\n",
    "# Compute masked pixels from mask ratio (same formula as N2V training)\n",
    "pn2v_patch_size = CONFIG['pn2v_patch_size']\n",
    "pn2v_num_masked = int(pn2v_patch_size * pn2v_patch_size * CONFIG['mask_ratio'])\n",
    "\n",
    "# Train PN2V - matching original structure\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING PN2V NETWORK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "pn2v_net_postfix = f\"pn2v_{CONFIG['data_name']}_bootstrap\"\n",
    "\n",
    "# Original MouseActin: numOfEpochs=200, stepsPerEpoch=5, batchSize=1, learningRate=1e-3\n",
    "trainHist, valHist = training.trainNetwork(\n",
    "    net=pn2v_net,\n",
    "    trainData=pn2v_train_data,\n",
    "    valData=pn2v_val_data,\n",
    "    postfix=pn2v_net_postfix,\n",
    "    directory=PN2V_RESULTS_PATH,\n",
    "    noiseModel=gmm_noise_model,\n",
    "    device=device,\n",
    "    numOfEpochs=CONFIG['pn2v_max_epochs'],           # 200 (matches original)\n",
    "    stepsPerEpoch=CONFIG['pn2v_steps_per_epoch'],    # 5 (matches original)\n",
    "    batchSize=CONFIG['pn2v_batch_size'],             # 1 (matches original)\n",
    "    patchSize=pn2v_patch_size,                       # 64\n",
    "    learningRate=CONFIG['pn2v_learning_rate'],       # 1e-3 (matches original)\n",
    "    virtualBatchSize=20,                             # 20 (matches original)\n",
    "    numMaskedPixels=pn2v_num_masked,                 # Explicit masked pixel count\n",
    "    maskPatchSize=CONFIG['mask_patch_size'],          # 7 (N2V-style 5x5+margin)\n",
    "    useZeroReplacement=True,                          # Replace masked pixels with 0\n",
    "    earlyStopPatience=CONFIG['pn2v_patience']        # Early stopping\n",
    ")\n",
    "\n",
    "# Plot PN2V training curves\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(trainHist, label='Training Loss', color='blue')\n",
    "plt.plot(valHist, label='Validation Loss', color='orange')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'PN2V Training ({len(trainHist)} epochs)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(os.path.join(PN2V_RESULTS_PATH, f'pn2v_training_curves_{pn2v_net_postfix}.png'), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úì PN2V training complete!\")\n",
    "print(f\"  Total epochs: {len(trainHist)}\")\n",
    "print(f\"  Best val loss: {min(valHist):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1291020b",
   "metadata": {},
   "source": [
    "## üîÆ Section 12: PN2V Prediction with Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b383b548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: prediction module already imported in Section 5\n",
    "\n",
    "# ============================================================\n",
    "# VERIFY REQUIRED VARIABLES EXIST\n",
    "# ============================================================\n",
    "if 'gmm_noise_model' not in dir():\n",
    "    raise RuntimeError(\"gmm_noise_model not found! Please run Section 10 first.\")\n",
    "if 'pn2v_net_postfix' not in dir():\n",
    "    raise RuntimeError(\"pn2v_net_postfix not found! Please run Section 11 first.\")\n",
    "\n",
    "# Load best PN2V model (add weights_only=False for PyTorch 2.6+)\n",
    "pn2v_net = torch.load(\n",
    "    os.path.join(PN2V_RESULTS_PATH, f'best_{pn2v_net_postfix}.net'),\n",
    "    map_location=device,\n",
    "    weights_only=False\n",
    ")\n",
    "pn2v_net = pn2v_net.to(device)\n",
    "pn2v_net.eval()\n",
    "\n",
    "print(f\"Loaded PN2V model from: {PN2V_RESULTS_PATH}\")\n",
    "print(f\"  Mean: {pn2v_net.mean:.2f}\")\n",
    "print(f\"  Std: {pn2v_net.std:.2f}\")\n",
    "\n",
    "# Prepare image for prediction\n",
    "noisy_for_pred = np.squeeze(noisy_image).astype(np.float32)\n",
    "\n",
    "print(\"\\nRunning PN2V prediction...\")\n",
    "print(\"  This generates MMSE estimate and prior mean\")\n",
    "\n",
    "# PN2V Prediction with tiled processing for large images\n",
    "# Adjust tile size based on image size\n",
    "tile_size = min(256, noisy_for_pred.shape[0], noisy_for_pred.shape[1])\n",
    "# Make tile_size divisible by 2^depth\n",
    "divisor = 2 ** CONFIG['pn2v_depth']\n",
    "tile_size = (tile_size // divisor) * divisor\n",
    "\n",
    "pn2v_prior_mean, pn2v_mmse = prediction.tiledPredict(\n",
    "    im=noisy_for_pred,\n",
    "    net=pn2v_net,\n",
    "    ps=tile_size,     # Tile size (must be divisible by 2^depth)\n",
    "    overlap=32,       # Overlap between tiles\n",
    "    noiseModel=gmm_noise_model,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì PN2V Prediction complete!\")\n",
    "print(f\"  Prior Mean shape: {pn2v_prior_mean.shape}\")\n",
    "print(f\"  MMSE shape: {pn2v_mmse.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c8d9bb",
   "metadata": {},
   "source": [
    "## üìà Section 13: Compute Uncertainty Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3c397b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute uncertainty from the network's sample outputs\n",
    "# Run network to get all samples\n",
    "noisy_norm = (noisy_for_pred - pn2v_net.mean) / pn2v_net.std\n",
    "\n",
    "# Pad for U-Net (must be divisible by 2^depth)\n",
    "H, W = noisy_norm.shape\n",
    "divisor = 2 ** CONFIG['pn2v_depth']  # Use actual depth from config\n",
    "pad_h = (divisor - H % divisor) % divisor\n",
    "pad_w = (divisor - W % divisor) % divisor\n",
    "\n",
    "if pad_h > 0 or pad_w > 0:\n",
    "    noisy_padded = np.pad(noisy_norm, ((0, pad_h), (0, pad_w)), mode='reflect')\n",
    "    print(f\"Padded image from {(H, W)} to {noisy_padded.shape}\")\n",
    "else:\n",
    "    noisy_padded = noisy_norm\n",
    "\n",
    "# Get samples from network\n",
    "input_tensor = torch.from_numpy(noisy_padded).unsqueeze(0).unsqueeze(0).float().to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    samples = pn2v_net(input_tensor) * 10.0  # Output scaling factor\n",
    "    \n",
    "# Denormalize samples\n",
    "samples_np = samples.cpu().numpy()[0]  # Shape: (num_samples, H_padded, W_padded)\n",
    "samples_denorm = samples_np * pn2v_net.std + pn2v_net.mean\n",
    "\n",
    "# Crop if padded to get back to original size\n",
    "if pad_h > 0 or pad_w > 0:\n",
    "    samples_denorm = samples_denorm[:, :H, :W]\n",
    "\n",
    "print(f\"Samples shape: {samples_denorm.shape}\")  # Should be (num_samples, H, W)\n",
    "\n",
    "# Compute uncertainty metrics\n",
    "# 1. Standard deviation across samples (epistemic uncertainty)\n",
    "std_map = np.std(samples_denorm, axis=0)\n",
    "\n",
    "# 2. Coefficient of variation (relative uncertainty)\n",
    "mean_map = np.mean(samples_denorm, axis=0)\n",
    "cv_map = std_map / (np.abs(mean_map) + 1e-8)\n",
    "\n",
    "# 3. Confidence interval width (95%)\n",
    "percentile_2_5 = np.percentile(samples_denorm, 2.5, axis=0)\n",
    "percentile_97_5 = np.percentile(samples_denorm, 97.5, axis=0)\n",
    "ci_width = percentile_97_5 - percentile_2_5\n",
    "\n",
    "print(\"‚úì Uncertainty maps computed!\")\n",
    "print(f\"  Number of samples: {samples_denorm.shape[0]}\")\n",
    "print(f\"  Standard deviation range: [{std_map.min():.2f}, {std_map.max():.2f}]\")\n",
    "print(f\"  95% CI width range: [{ci_width.min():.2f}, {ci_width.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5760ed51",
   "metadata": {},
   "source": [
    "## üé® Section 14: Visualize Uncertainty Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077b3949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize uncertainty maps\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Row 1: Denoised results\n",
    "im0 = axes[0, 0].imshow(noisy_image, cmap='magma')\n",
    "axes[0, 0].set_title('Noisy Input', fontsize=12)\n",
    "axes[0, 0].axis('off')\n",
    "plt.colorbar(im0, ax=axes[0, 0], fraction=0.046)\n",
    "\n",
    "im1 = axes[0, 1].imshow(pn2v_prior_mean, cmap='magma')\n",
    "axes[0, 1].set_title('PN2V Prior Mean', fontsize=12)\n",
    "axes[0, 1].axis('off')\n",
    "plt.colorbar(im1, ax=axes[0, 1], fraction=0.046)\n",
    "\n",
    "im2 = axes[0, 2].imshow(pn2v_mmse, cmap='magma')\n",
    "axes[0, 2].set_title('PN2V MMSE (Best Estimate)', fontsize=12)\n",
    "axes[0, 2].axis('off')\n",
    "plt.colorbar(im2, ax=axes[0, 2], fraction=0.046)\n",
    "\n",
    "# Row 2: Uncertainty maps\n",
    "im3 = axes[1, 0].imshow(std_map, cmap='hot')\n",
    "axes[1, 0].set_title('Uncertainty: Std Deviation', fontsize=12)\n",
    "axes[1, 0].axis('off')\n",
    "plt.colorbar(im3, ax=axes[1, 0], fraction=0.046)\n",
    "\n",
    "im4 = axes[1, 1].imshow(cv_map, cmap='hot', vmin=0, vmax=np.percentile(cv_map, 99))\n",
    "axes[1, 1].set_title('Uncertainty: Coeff. of Variation', fontsize=12)\n",
    "axes[1, 1].axis('off')\n",
    "plt.colorbar(im4, ax=axes[1, 1], fraction=0.046)\n",
    "\n",
    "im5 = axes[1, 2].imshow(ci_width, cmap='hot')\n",
    "axes[1, 2].set_title('Uncertainty: 95% CI Width', fontsize=12)\n",
    "axes[1, 2].axis('off')\n",
    "plt.colorbar(im5, ax=axes[1, 2], fraction=0.046)\n",
    "\n",
    "plt.suptitle('PN2V Results with Uncertainty Quantification', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(PN2V_RESULTS_PATH, f'pn2v_uncertainty_{CONFIG[\"data_name\"]}.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9f5222",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è Section 15: Compare N2V vs PN2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac71219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side comparison of N2V vs PN2V\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "\n",
    "# Row 1: Full images\n",
    "im0 = axes[0, 0].imshow(noisy_image, cmap='magma')\n",
    "axes[0, 0].set_title('Noisy Input', fontsize=12)\n",
    "axes[0, 0].axis('off')\n",
    "plt.colorbar(im0, ax=axes[0, 0], fraction=0.046)\n",
    "\n",
    "im1 = axes[0, 1].imshow(n2v_denoised, cmap='magma')\n",
    "axes[0, 1].set_title('N2V Optimized', fontsize=12)\n",
    "axes[0, 1].axis('off')\n",
    "plt.colorbar(im1, ax=axes[0, 1], fraction=0.046)\n",
    "\n",
    "im2 = axes[0, 2].imshow(pn2v_mmse, cmap='magma')\n",
    "axes[0, 2].set_title('PN2V MMSE', fontsize=12)\n",
    "axes[0, 2].axis('off')\n",
    "plt.colorbar(im2, ax=axes[0, 2], fraction=0.046)\n",
    "\n",
    "# Difference between methods\n",
    "method_diff = n2v_denoised - pn2v_mmse\n",
    "vmax_diff = np.abs(method_diff).max()\n",
    "im3 = axes[0, 3].imshow(method_diff, cmap='RdBu_r', vmin=-vmax_diff, vmax=vmax_diff)\n",
    "axes[0, 3].set_title('N2V - PN2V Difference', fontsize=12)\n",
    "axes[0, 3].axis('off')\n",
    "plt.colorbar(im3, ax=axes[0, 3], fraction=0.046)\n",
    "\n",
    "# Row 2: Zoomed comparison\n",
    "h, w = noisy_image.shape\n",
    "crop_size = min(256, h//2, w//2)\n",
    "cy, cx = h//2, w//2\n",
    "y1, y2 = cy - crop_size//2, cy + crop_size//2\n",
    "x1, x2 = cx - crop_size//2, cx + crop_size//2\n",
    "\n",
    "axes[1, 0].imshow(noisy_image[y1:y2, x1:x2], cmap='magma')\n",
    "axes[1, 0].set_title('Noisy (Zoomed)', fontsize=12)\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "axes[1, 1].imshow(n2v_denoised[y1:y2, x1:x2], cmap='magma')\n",
    "axes[1, 1].set_title('N2V (Zoomed)', fontsize=12)\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "axes[1, 2].imshow(pn2v_mmse[y1:y2, x1:x2], cmap='magma')\n",
    "axes[1, 2].set_title('PN2V MMSE (Zoomed)', fontsize=12)\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "axes[1, 3].imshow(std_map[y1:y2, x1:x2], cmap='hot')\n",
    "axes[1, 3].set_title('Uncertainty (Zoomed)', fontsize=12)\n",
    "axes[1, 3].axis('off')\n",
    "\n",
    "plt.suptitle('N2V vs PN2V Comparison', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(COMPARISON_PATH, f'n2v_vs_pn2v_{CONFIG[\"data_name\"]}.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print comparison statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nN2V Optimized:\")\n",
    "print(f\"  Mean: {n2v_denoised.mean():.2f}\")\n",
    "print(f\"  Std: {n2v_denoised.std():.2f}\")\n",
    "print(f\"\\nPN2V MMSE:\")\n",
    "print(f\"  Mean: {pn2v_mmse.mean():.2f}\")\n",
    "print(f\"  Std: {pn2v_mmse.std():.2f}\")\n",
    "print(f\"\\nMethod Difference (N2V - PN2V):\")\n",
    "print(f\"  Mean absolute diff: {np.abs(method_diff).mean():.4f}\")\n",
    "print(f\"  Max absolute diff: {np.abs(method_diff).max():.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081a4306",
   "metadata": {},
   "source": [
    "## üíæ Section 16: Save All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0aecd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save PN2V results\n",
    "data_name = CONFIG['data_name']\n",
    "\n",
    "# PN2V denoised images\n",
    "tifffile.imwrite(os.path.join(PN2V_RESULTS_PATH, f'pn2v_prior_mean_{data_name}.tif'), \n",
    "                 pn2v_prior_mean.astype(np.float32))\n",
    "tifffile.imwrite(os.path.join(PN2V_RESULTS_PATH, f'pn2v_mmse_{data_name}.tif'), \n",
    "                 pn2v_mmse.astype(np.float32))\n",
    "\n",
    "# Uncertainty maps\n",
    "tifffile.imwrite(os.path.join(PN2V_RESULTS_PATH, f'uncertainty_std_{data_name}.tif'), \n",
    "                 std_map.astype(np.float32))\n",
    "tifffile.imwrite(os.path.join(PN2V_RESULTS_PATH, f'uncertainty_cv_{data_name}.tif'), \n",
    "                 cv_map.astype(np.float32))\n",
    "tifffile.imwrite(os.path.join(PN2V_RESULTS_PATH, f'uncertainty_ci95_{data_name}.tif'), \n",
    "                 ci_width.astype(np.float32))\n",
    "\n",
    "# Comparison results\n",
    "tifffile.imwrite(os.path.join(COMPARISON_PATH, f'n2v_denoised_{data_name}.tif'), \n",
    "                 n2v_denoised.astype(np.float32))\n",
    "tifffile.imwrite(os.path.join(COMPARISON_PATH, f'pn2v_mmse_{data_name}.tif'), \n",
    "                 pn2v_mmse.astype(np.float32))\n",
    "tifffile.imwrite(os.path.join(COMPARISON_PATH, f'method_difference_{data_name}.tif'), \n",
    "                 method_diff.astype(np.float32))\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"‚úÖ ALL RESULTS SAVED\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüìÅ N2V Results: {N2V_RESULTS_PATH}\")\n",
    "for f in os.listdir(N2V_RESULTS_PATH):\n",
    "    size_kb = os.path.getsize(os.path.join(N2V_RESULTS_PATH, f)) / 1024\n",
    "    print(f\"   üìÑ {f} ({size_kb:.1f} KB)\")\n",
    "\n",
    "print(f\"\\nüìÅ PN2V Results: {PN2V_RESULTS_PATH}\")\n",
    "for f in os.listdir(PN2V_RESULTS_PATH):\n",
    "    size_kb = os.path.getsize(os.path.join(PN2V_RESULTS_PATH, f)) / 1024\n",
    "    print(f\"   üìÑ {f} ({size_kb:.1f} KB)\")\n",
    "\n",
    "print(f\"\\nüìÅ Comparison: {COMPARISON_PATH}\")\n",
    "for f in os.listdir(COMPARISON_PATH):\n",
    "    size_kb = os.path.getsize(os.path.join(COMPARISON_PATH, f)) / 1024\n",
    "    print(f\"   üìÑ {f} ({size_kb:.1f} KB)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23049cc5",
   "metadata": {},
   "source": [
    "## üì§ Section 17: Commit to GitHub (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dccdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this cell if you want to push changes to GitHub\n",
    "# You'll need to authenticate with your GitHub token\n",
    "\n",
    "RUN_GIT_PUSH = False  # Change to True to enable\n",
    "\n",
    "if RUN_GIT_PUSH:\n",
    "    from getpass import getpass\n",
    "    \n",
    "    # Get credentials\n",
    "    GITHUB_USERNAME = input(\"GitHub username: \")\n",
    "    GITHUB_TOKEN = getpass(\"GitHub token (hidden): \")\n",
    "    \n",
    "    os.chdir(REPO_PATH)\n",
    "    \n",
    "    # Configure git\n",
    "    subprocess.run(['git', 'config', 'user.email', f'{GITHUB_USERNAME}@users.noreply.github.com'])\n",
    "    subprocess.run(['git', 'config', 'user.name', GITHUB_USERNAME])\n",
    "    \n",
    "    # Set remote with authentication\n",
    "    auth_url = f'https://{GITHUB_USERNAME}:{GITHUB_TOKEN}@github.com/ZurvanAkarna/PPN2V.git'\n",
    "    subprocess.run(['git', 'remote', 'set-url', 'origin', auth_url])\n",
    "    \n",
    "    # Check for changes\n",
    "    result = subprocess.run(['git', 'status', '--porcelain'], capture_output=True, text=True)\n",
    "    if result.stdout.strip():\n",
    "        print(\"Changes detected:\")\n",
    "        print(result.stdout)\n",
    "        \n",
    "        # Add and commit\n",
    "        subprocess.run(['git', 'add', '-A'])\n",
    "        commit_msg = f\"N2V+PN2V pipeline: {CONFIG['data_name']}\"\n",
    "        subprocess.run(['git', 'commit', '-m', commit_msg])\n",
    "        subprocess.run(['git', 'push', 'origin', 'main'])\n",
    "        print(\"\\n‚úì Changes pushed to GitHub!\")\n",
    "    else:\n",
    "        print(\"No changes to commit.\")\n",
    "else:\n",
    "    print(\"Git push disabled. Set RUN_GIT_PUSH = True to enable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcee600",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Pipeline Complete!\n",
    "\n",
    "You have successfully run the complete N2V ‚Üí PN2V pipeline:\n",
    "\n",
    "### Stage 1: N2V Optimized ‚úÖ\n",
    "- Fast denoising with optimized blind-spot configuration\n",
    "- Results in: `Google Drive/MyDrive/PPN2V/results/n2v_optimized/`\n",
    "\n",
    "### Stage 2: PN2V Bootstrap ‚úÖ\n",
    "- Used N2V output as signal estimate for noise model\n",
    "- Trained GMM noise model + PN2V network\n",
    "- Results in: `Google Drive/MyDrive/PPN2V/results/pn2v_bootstrap/`\n",
    "\n",
    "### Comparison & Uncertainty ‚úÖ\n",
    "- Side-by-side N2V vs PN2V comparison\n",
    "- Uncertainty maps (Std Dev, Coeff. of Variation, 95% CI)\n",
    "- Results in: `Google Drive/MyDrive/PPN2V/results/comparison/`\n",
    "\n",
    "---\n",
    "\n",
    "### Output Files Summary:\n",
    "\n",
    "| Directory | File | Description |\n",
    "|-----------|------|-------------|\n",
    "| `n2v_optimized/` | `n2v_denoised_*.tif` | N2V denoised image |\n",
    "| `n2v_optimized/` | `best_*.pth` | Best N2V model |\n",
    "| `pn2v_bootstrap/` | `pn2v_mmse_*.tif` | PN2V MMSE estimate |\n",
    "| `pn2v_bootstrap/` | `uncertainty_std_*.tif` | Standard deviation map |\n",
    "| `pn2v_bootstrap/` | `uncertainty_cv_*.tif` | Coefficient of variation map |\n",
    "| `pn2v_bootstrap/` | `uncertainty_ci95_*.tif` | 95% confidence interval width |\n",
    "| `comparison/` | `n2v_vs_pn2v_*.png` | Visual comparison |\n",
    "| `comparison/` | `method_difference_*.tif` | Difference between methods |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
